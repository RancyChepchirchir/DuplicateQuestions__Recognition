{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "QuestionsDuplicates.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iW3NlTbZ29bg"
      },
      "source": [
        "**Initialization**\n",
        "* I use these 3 lines of code on top of my each Notebooks because it will help to prevent any problems while reloading and reworking on a Project or Problem. And the third line of code helps to make visualization within the Notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXIQMMwC1xYs"
      },
      "source": [
        "#@ Initialization:\n",
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OuNgk63U_hBA"
      },
      "source": [
        "**Downloading the Dependencies**\n",
        "* I have downloaded all the Libraries and Dependencies required for this Project in one particular cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4UdZnqc3PNB"
      },
      "source": [
        "#@ Downloading the Libraries and Dependencies. \n",
        "# !pip install -q -U trax                         # Downloading the Trax.\n",
        "# nltk.download(\"punkt\")\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import nltk\n",
        "import trax\n",
        "from trax import layers as tl\n",
        "from trax.supervised import training\n",
        "from trax.fastmath import numpy as fastnp\n",
        "import random\n",
        "from collections import defaultdict\n",
        "from functools import partial\n",
        "\n",
        "random.seed(111)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBBqIIViCPbp"
      },
      "source": [
        "**Getting the Data**\n",
        "* I have used Google Colab for this Project so the process of downloading and reading the Data might be different in other platforms. I will be using **Quora Answer Question Dataset** for this Project. I will build a Model that can Identify the Similar Questions or the Duplicate Questions which is useful when we have to work with several versions of the same Questions. The Dataset is labeled."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1QazSnI__Ko",
        "outputId": "ffc29f63-dbb2-4fa4-e970-5847ac7849de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        }
      },
      "source": [
        "#@ Getting the Data:\n",
        "PATH = \"/content/drive/My Drive/Colab Notebooks/Questions\"\n",
        "data = pd.read_csv(os.path.join(PATH, \"Questions.zip\"))\n",
        "\n",
        "#@ Inspecting the Data:\n",
        "print(f\"Number of Questions Pairs: {len(data)}\")\n",
        "data.head(10)                                                        # Inspecting the DataFrame."
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Questions Pairs: 404351\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>qid1</th>\n",
              "      <th>qid2</th>\n",
              "      <th>question1</th>\n",
              "      <th>question2</th>\n",
              "      <th>is_duplicate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>What is the step by step guide to invest in sh...</td>\n",
              "      <td>What is the step by step guide to invest in sh...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
              "      <td>What would happen if the Indian government sto...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>How can I increase the speed of my internet co...</td>\n",
              "      <td>How can Internet speed be increased by hacking...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
              "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>9</td>\n",
              "      <td>10</td>\n",
              "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
              "      <td>Which fish would survive in salt water?</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>11</td>\n",
              "      <td>12</td>\n",
              "      <td>Astrology: I am a Capricorn Sun Cap moon and c...</td>\n",
              "      <td>I'm a triple Capricorn (Sun, Moon and ascendan...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>13</td>\n",
              "      <td>14</td>\n",
              "      <td>Should I buy tiago?</td>\n",
              "      <td>What keeps childern active and far from phone ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7</td>\n",
              "      <td>15</td>\n",
              "      <td>16</td>\n",
              "      <td>How can I be a good geologist?</td>\n",
              "      <td>What should I do to be a great geologist?</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>8</td>\n",
              "      <td>17</td>\n",
              "      <td>18</td>\n",
              "      <td>When do you use シ instead of し?</td>\n",
              "      <td>When do you use \"&amp;\" instead of \"and\"?</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>9</td>\n",
              "      <td>19</td>\n",
              "      <td>20</td>\n",
              "      <td>Motorola (company): Can I hack my Charter Moto...</td>\n",
              "      <td>How do I hack Motorola DCX3400 for free internet?</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id  qid1  ...                                          question2 is_duplicate\n",
              "0   0     1  ...  What is the step by step guide to invest in sh...            0\n",
              "1   1     3  ...  What would happen if the Indian government sto...            0\n",
              "2   2     5  ...  How can Internet speed be increased by hacking...            0\n",
              "3   3     7  ...  Find the remainder when [math]23^{24}[/math] i...            0\n",
              "4   4     9  ...            Which fish would survive in salt water?            0\n",
              "5   5    11  ...  I'm a triple Capricorn (Sun, Moon and ascendan...            1\n",
              "6   6    13  ...  What keeps childern active and far from phone ...            0\n",
              "7   7    15  ...          What should I do to be a great geologist?            1\n",
              "8   8    17  ...              When do you use \"&\" instead of \"and\"?            0\n",
              "9   9    19  ...  How do I hack Motorola DCX3400 for free internet?            0\n",
              "\n",
              "[10 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMKciVITGpvF"
      },
      "source": [
        "**Processing the Data**\n",
        "* I will split the Data into Training set and Testing Set. The Test Set will be used later to evaluate the Model. I will select only the Question Pairs that are duplicate to train the Model. I will build two batches as input for the Neural Networks: Siamese Networks. The Test set uses the original pairs of Questions and the Status describing if the Questions are duplicates. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIliyuetFyzY",
        "outputId": "ac90b176-b8c5-4de0-c052-35f923ba32c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "#@ Processing the Data:\n",
        "N_train = 300000                                               \n",
        "N_test = 10240                                                 \n",
        "data_train = data[:N_train]                                                    # Training pairs.\n",
        "data_test = data[N_train:N_train+N_test]                                       # Test pairs.\n",
        "del(data)                                                                      # Removing.\n",
        "\n",
        "#@ Inspecting the Data:\n",
        "print(f\"Training Set: {len(data_train)} and Test Set: {len(data_test)}\")\n",
        "\n",
        "#@ Selecting the Question Pairs for Training:\n",
        "train_idx = (data_train[\"is_duplicate\"] == 1).to_numpy()\n",
        "train_idx = [i for i,x in enumerate(train_idx) if x]\n",
        "print(f\"Number of Duplicate Questions: {len(train_idx)}\")\n",
        "print(f\"Indexes of first Duplicate Questions: {train_idx[:10]}\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Set: 300000 and Test Set: 10240\n",
            "Number of Duplicate Questions: 111486\n",
            "Indexes of first Duplicate Questions: [5, 7, 11, 12, 13, 15, 16, 18, 20, 29]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdCp1rDSOuod",
        "outputId": "908d6774-0780-4f86-843f-6f6b48aad63b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "#@ Inspecting the Duplicate Questions:\n",
        "print(data_train[\"question1\"][20])                                 # Index 20 has Duplicate Questions pairs.\n",
        "print(data_train[\"question2\"][20])                                 # Index 20 has Duplicate Questions pairs.\n",
        "print(\"Index 20 is duplicate:\", data_train[\"is_duplicate\"][20])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Why do rockets look white?\n",
            "Why are rockets and boosters painted white?\n",
            "Index 20 is duplicate: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpA5u5orQRDN"
      },
      "source": [
        "**Preparing the Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JREoTE1JMQnZ",
        "outputId": "c57203d1-2736-4aff-bb22-22036e30d06b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "#@ Preparing the Data: Training the Model:\n",
        "Q1_train_words = np.array(data_train[\"question1\"][train_idx])\n",
        "Q2_train_words = np.array(data_train[\"question2\"][train_idx])\n",
        "\n",
        "#@ Preparing the Data: Evaluating the Model:\n",
        "Q1_test_words = np.array(data_test[\"question1\"])\n",
        "Q2_test_words = np.array(data_test[\"question2\"])\n",
        "y_test = np.array(data_test[\"is_duplicate\"])\n",
        "\n",
        "#@ Inspecting the Data:\n",
        "print(\"TRAINING QUESTIONS:\\n\")\n",
        "print(\"Question 1:\", Q1_train_words[7])\n",
        "print(\"Question 2:\", Q2_train_words[7], \"\\n\")\n",
        "\n",
        "print(\"TESTING QUESTIONS:\\n\")\n",
        "print(\"Question 1:\", Q1_test_words[7])\n",
        "print(\"Question 2:\", Q2_test_words[7], \"\\n\")\n",
        "print(\"Inspecting Testing pairs is duplicate:\", y_test[0])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TRAINING QUESTIONS:\n",
            "\n",
            "Question 1: Why are so many Quora users posting questions that are readily answered on Google?\n",
            "Question 2: Why do people ask Quora questions which can be answered easily by Google? \n",
            "\n",
            "TESTING QUESTIONS:\n",
            "\n",
            "Question 1: Which is the best digital photo frame?\n",
            "Question 2: What are the best 12-inch digital photo frames? \n",
            "\n",
            "Inspecting Testing pairs is duplicate: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-I0nWKTUqSI"
      },
      "source": [
        "**Preparing the Data**\n",
        "* I will encode each word of the selected pairs with an Index which will be a list of numbers. Firstly, I will Tokenize each word using NLTK and I will use Python's Default Dictionary which assigns the values 0 to all Out of Vocabulary Words. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GE6ELe7UBY9",
        "outputId": "0381ab42-f9ed-4da0-e06b-a18f1b59c544",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "#@ Preparing the Data:\n",
        "Q1_train = np.empty_like(Q1_train_words)                                # Creating new Training array.\n",
        "Q2_train = np.empty_like(Q2_train_words)                                # Creating new Training array.\n",
        "Q1_test = np.empty_like(Q1_test_words)                                  # Creating new Test array.\n",
        "Q2_test = np.empty_like(Q2_test_words)                                  # Creating new Test array.\n",
        "\n",
        "#@ Building Vocabulary with Training Dataset:\n",
        "vocab = defaultdict(lambda: 0)\n",
        "vocab[\"<PAD>\"] = 1\n",
        "for idx in range(len(Q1_train_words)):\n",
        "  Q1_train[idx] = nltk.word_tokenize(Q1_train_words[idx])               # Tokenizing the Training Set.\n",
        "  Q2_train[idx] = nltk.word_tokenize(Q2_train_words[idx])               # Tokenizing the Training Set.\n",
        "  q = Q1_train[idx] + Q2_train[idx]\n",
        "  for word in q:\n",
        "    if word not in vocab:\n",
        "      vocab[word] = len(vocab) + 1\n",
        "print(\"The length of the Vocabulary is:\", len(vocab))\n",
        "\n",
        "#@ Testing Dataset:\n",
        "for idx in range(len(Q1_test_words)):\n",
        "  Q1_test[idx] = nltk.word_tokenize(Q1_test_words[idx])                 # Tokenizing the Test Set.\n",
        "  Q2_test[idx] = nltk.word_tokenize(Q2_test_words[idx])                 # Tokenizing the Test Set.\n",
        "\n",
        "#@ Inspecting the Final Prepared Dataset:\n",
        "print(\"Training Set is reduced to:\", len(Q1_train))\n",
        "print(\"Test Set is:\", len(Q1_test))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The length of the Vocabulary is: 36342\n",
            "Training Set is reduced to: 111486\n",
            "Test Set is: 10240\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ywba7dxVQu_l"
      },
      "source": [
        "**Preparing the Data**\n",
        "* I will convert each Questions Pairs to Tensors or array of Numbers using the Vocabulary. Then I will split the Training set into Training and Validation Dataset so that I can use it to train and evaluate the Neural Networks: Siamese Networks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVXqncSH1ntc",
        "outputId": "d943a1a8-e038-42a1-b0a0-06419833e2e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "#@ Preparing the Data:\n",
        "\n",
        "#@ Converting Questions pairs to array of Integers:\n",
        "for i in range(len(Q1_train)):\n",
        "  Q1_train[i] = [vocab[word] for word in Q1_train[i]]\n",
        "  Q2_train[i] = [vocab[word] for word in Q2_train[i]]\n",
        "\n",
        "#@ Converting Questions pairs to array of Integers:\n",
        "for i in range(len(Q1_test)):\n",
        "  Q1_test[i] = [vocab[word] for word in Q1_test[i]]\n",
        "  Q2_test[i] = [vocab[word] for word in Q2_test[i]]\n",
        "\n",
        "#@ Inspecting the Encoded Data:\n",
        "print(\"Question in the Training Set:\")                           # Inspecting the Training Set.\n",
        "print(Q1_train_words[7], \"\\n\")\n",
        "print(\"Encoded Version:\")\n",
        "print(Q1_train[7], \"\\n\")\n",
        "print(\"Question in the Test Set:\")                               # Inspecting the Test Set.\n",
        "print(Q1_test_words[7], \"\\n\")\n",
        "print(\"Encoded Version:\")\n",
        "print(Q1_test[7], \"\\n\")\n",
        "\n",
        "#@ Splitting the Training Set into Training and Validation Dataset:\n",
        "split = int(len(Q1_train) * 0.8)\n",
        "train_Q1, train_Q2 = Q1_train[:split], Q2_train[:split]                        # Split for Training set.\n",
        "val_Q1, val_Q2 = Q1_train[split:], Q2_train[split:]                            # Split for Validation set.\n",
        "print(f\"Total numbers of questions pairs: {len(Q1_train)}\")              \n",
        "print(f\"The length of Training set: {len(train_Q1)}\")                          # Length of Final Training set.\n",
        "print(f\"The length of Validation set: {len(val_Q1)}\")                          # Length of Final Validation set."
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Question in the Training Set:\n",
            "Why are so many Quora users posting questions that are readily answered on Google? \n",
            "\n",
            "Encoded Version:\n",
            "[86, 87, 88, 89, 90, 91, 92, 93, 17, 87, 94, 95, 72, 96, 21] \n",
            "\n",
            "Question in the Test Set:\n",
            "Which is the best digital photo frame? \n",
            "\n",
            "Encoded Version:\n",
            "[283, 156, 78, 216, 1442, 1223, 4114, 21] \n",
            "\n",
            "Total numbers of questions pairs: 111486\n",
            "The length of Training set: 89188\n",
            "The length of Validation set: 22298\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxVe0OX0XNeE"
      },
      "source": [
        "**Data Generator**\n",
        "* In most of the Natural Language Processing and AI in general, using batches when training the Dataset is more efficient. Now, I will build the Data Generator that takes in Questions pairs and returns batches in the form of Tuples. The Tuples consist of two arrays and each array will have batch size Questions pairs. The command next(data generator) will return the next batch. The Data Generator will returns the Data in a format that can be used directly int the Model while computing Feed Forward. It will return a pair of arrays of Questions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBN7QM5TWv49",
        "outputId": "f53f2b18-40ea-4c2c-e46f-3f81b868de1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "#@ Data Generator:\n",
        "def data_generator(Q1, Q2, batch_size, pad=1, shuffle=True):\n",
        "  \"\"\" Generator Function that yields the Batches of Data. \"\"\"\n",
        "  #@ Initializing the Dependencies:\n",
        "  input1, input2 = [], []\n",
        "  idx = 0\n",
        "  len_q = len(Q1)\n",
        "  question_index = [*range(len_q)]\n",
        "  if shuffle:\n",
        "    random.shuffle(question_index)\n",
        "  \n",
        "  while True:\n",
        "    if idx >= len_q:\n",
        "      idx = 0\n",
        "      if shuffle:\n",
        "        random.shuffle(question_index)\n",
        "    #@ Getting the Questions pairs in Index positions:\n",
        "    q1 = Q1[question_index[idx]]\n",
        "    q2 = Q2[question_index[idx]]\n",
        "    idx += 1\n",
        "    #@ Adding the Data:\n",
        "    input1.append(q1)\n",
        "    input2.append(q2)\n",
        "    if len(input1) == batch_size:\n",
        "      max_len = max(max([len(q) for q in input1]),\n",
        "                    max([len(q) for q in input2]))\n",
        "      max_len = 2**int(np.ceil(np.log2(max_len)))\n",
        "      b1, b2 = [], []\n",
        "      for q1, q2 in zip(input1, input2):\n",
        "        q1 = q1 + [pad] * (max_len - len(q1))                         # Adding pad to q1 until it reaches max length.\n",
        "        q2 = q2 + [pad] * (max_len - len(q2))                         # Adding pad to q2 until it reaches max length.\n",
        "        b1.append(q1)\n",
        "        b2.append(q2)\n",
        "      yield np.array(b1), np.array(b2)\n",
        "      input1, input2 = [], []                                         # Resetting the Batches.\n",
        "\n",
        "#@ Inspecting the Example of Data Generator:\n",
        "res1, res2 = next(data_generator(train_Q1, train_Q2, batch_size=2))\n",
        "print(f\"First Questions:\\n{res1}\")\n",
        "print(f\"\\nSecond Questions:\\n{res2}\")"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First Questions:\n",
            "[[  32   33    4  107   65  139  115  129 1355   21    1    1    1    1\n",
            "     1    1]\n",
            " [  32   33    4   49  575   72 1093   21    1    1    1    1    1    1\n",
            "     1    1]]\n",
            "\n",
            "Second Questions:\n",
            "[[  32   33    4  107   65  139  129 1355   21    1    1    1    1    1\n",
            "     1    1]\n",
            " [  32   38    4   49  575  127 1093   21    1    1    1    1    1    1\n",
            "     1    1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRCml-SKh3gY"
      },
      "source": [
        "**Siamese Neural Network**\n",
        "* A Siamese Neural Network is a Neural Network which uses the same weight while working in tandem on two different Input vectors to compute comparable output Vectors. Here, I will get the Embedding, run it through LSTM or Long Short Term Memory Network, Noramlize the two Vectors and Finally, I will use Triplet Loss to get the corresponding Cosine Similarity for each pair of Questions. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MB-pbo2e7b4",
        "outputId": "06344679-2c11-49ed-fbef-0330b30f292e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "#@ Siamese Neural Network using Trax:\n",
        "def Siamese(vocab_size=len(vocab), d_model=128, mode=\"train\"):\n",
        "  \"\"\" Returns a Siamese Model. \"\"\"\n",
        "  #@ Normalizing the Vectors for L2 Normalization:\n",
        "  def normalize(x):\n",
        "    return x / fastnp.sqrt(fastnp.sum(x*x, axis=-1, keepdims=True))\n",
        "  #@ Preparing the Model:\n",
        "  processor = tl.Serial(                                                  # Returns one hot Vector.\n",
        "      tl.Embedding(vocab_size=vocab_size, d_feature=d_model),             # Adding Embedding Layer.\n",
        "      tl.LSTM(n_units=d_model),                                           # Adding the LSTM Layer.\n",
        "      tl.Mean(axis=1),                                                    # Mean over Columns in Neural Networks.\n",
        "      # tl.Dense(n_units=vocab_size),                                     # Adding a Dense Layer.\n",
        "      tl.Fn(\"Normalize\", lambda x: normalize(x))                          # Adding the Normalizing Function.\n",
        "  )\n",
        "  #@ Running the Model in parallel:\n",
        "  model = tl.Parallel(processor, processor)\n",
        "  return model\n",
        "\n",
        "#@ Setting up Siamese Neural Network Model:\n",
        "model = Siamese()\n",
        "print(model)                                                              # Inspecting the Model."
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parallel_in2_out2[\n",
            "  Serial[\n",
            "    Embedding_41789_128\n",
            "    LSTM_128\n",
            "    Mean\n",
            "    Normalize\n",
            "  ]\n",
            "  Serial[\n",
            "    Embedding_41789_128\n",
            "    LSTM_128\n",
            "    Mean\n",
            "    Normalize\n",
            "  ]\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DmBELaQ0hGA"
      },
      "source": [
        "**Triplet Loss**\n",
        "* The Triplet Loss makes use of a Baseline or Anchor Input which is compared to the Positive or Truthy Input and a Negatve or Falsy Input. The distance from the Anchor Input to the Positive Input is minimized and the distance from the Anchor Input to the Negative Input is maximized. The Triplet Loss is composed of two terms where one term utilizes the mean of all the non duplicates and the second term utilizes the Closest Negative. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5or7rjVz4W_"
      },
      "source": [
        "#@ Triplet Loss Function:\n",
        "def TripletLossFn(v1, v2, margin=0.25):\n",
        "  \"\"\" Custom Loss Function. \"\"\"\n",
        "  scores = fastnp.dot(v1, v2.T)                                                       # Calculating the dot product of two batches.\n",
        "  batch_size = len(scores)                                                            # Calculating the new batch size.\n",
        "  positive = fastnp.diagonal(scores)                                                  # Getting positive diagonal entries in scores.\n",
        "  negative_without_positive = scores - 2.0 * fastnp.eye(batch_size)\n",
        "  closest_negative = negative_without_positive.max(axis=1)                            # Taking row by row max.\n",
        "  negative_zero_on_duplicate = scores * (1.0 - fastnp.eye(batch_size))\n",
        "  mean_negative = fastnp.sum(negative_zero_on_duplicate, axis=1)/(batch_size - 1)\n",
        "  triplet_loss1 = fastnp.maximum(0, margin - positive + closest_negative)\n",
        "  triplet_loss2 = fastnp.maximum(0, margin - positive + mean_negative)\n",
        "  triplet_loss = fastnp.mean(triplet_loss1 + triplet_loss2)\n",
        "  return triplet_loss\n",
        "\n",
        "#@ Triplet Loss:\n",
        "def TripletLoss(margin=0.25):\n",
        "  triplet_loss_fn = partial(TripletLossFn, margin=margin)\n",
        "  return tl.Fn(\"TripletLoss\", triplet_loss_fn)"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THnc2u46Frcg"
      },
      "source": [
        "**Training the Model**\n",
        "* Now, I will train the Model. I will define the Cost Function and the Optimizer as ususal. I will use Training Iterator to go through all the Data for each Epochs while training the Model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJOlFJpl9SnB",
        "outputId": "6f351386-1d2d-428d-c8ab-a753703a5ae6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        }
      },
      "source": [
        "#@ Preparing the Data:\n",
        "batch_size = 256\n",
        "train_generator = data_generator(train_Q1, train_Q2, batch_size, vocab[\"<PAD>\"])\n",
        "val_generator = data_generator(val_Q1, val_Q2, batch_size, vocab[\"<PAD>\"])\n",
        "\n",
        "#@ Training the Model:\n",
        "lr_schedule = trax.lr.warmup_and_rsqrt_decay(400, 0.01)\n",
        "def train_model(Siamese, TripletLoss, lr_schedule, train_generator=train_generator,\n",
        "                val_generator=val_generator, output_dir=\"model/\"):\n",
        "  \"\"\" Training the Siamese Model. \"\"\"\n",
        "  output_dir = os.path.expanduser(output_dir)\n",
        "  \n",
        "  #@ Training:\n",
        "  train_task = training.TrainTask(\n",
        "      labeled_data = train_generator,                                                   # Using Train Generator.\n",
        "      loss_layer = TripletLoss(),                                                       # Using Triplet Loss Function.\n",
        "      optimizer = trax.optimizers.Adam(0.001),                                          # Using Adam Optimizer.\n",
        "      lr_schedule = lr_schedule                                                         # Using Trax Multifactor Schedule Function.\n",
        "  )\n",
        "  #@ Evaluating:\n",
        "  eval_task = training.EvalTask(\n",
        "      labeled_data = val_generator,                                                     # Using Validation Generator.\n",
        "      metrics = [TripletLoss()],                                                        # Instantiating the Objects for Evaluation.\n",
        "      n_eval_batches = 3\n",
        "  )\n",
        "  #@ Training the Model:\n",
        "  training_loop = training.Loop(                                                        # Training the Model\n",
        "      Siamese(),                                                                        # Siameses Neural Networks.\n",
        "      train_task, eval_tasks = eval_task,\n",
        "      output_dir = output_dir\n",
        "  )\n",
        "  return training_loop\n",
        "\n",
        "#@ Training the Model:\n",
        "training_loop = train_model(Siamese, TripletLoss, lr_schedule)\n",
        "training_loop.run(1000)                                                                  # Training the Model for 1000 Epochs."
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Step   1100: Ran 100 train steps in 89.55 secs\n",
            "Step   1100: train TripletLoss |  0.25932726\n",
            "Step   1100: eval  TripletLoss |  0.24985564\n",
            "\n",
            "Step   1200: Ran 100 train steps in 79.07 secs\n",
            "Step   1200: train TripletLoss |  0.20025587\n",
            "Step   1200: eval  TripletLoss |  0.16663148\n",
            "\n",
            "Step   1300: Ran 100 train steps in 76.26 secs\n",
            "Step   1300: train TripletLoss |  0.13685444\n",
            "Step   1300: eval  TripletLoss |  0.12138339\n",
            "\n",
            "Step   1400: Ran 100 train steps in 77.77 secs\n",
            "Step   1400: train TripletLoss |  0.10340497\n",
            "Step   1400: eval  TripletLoss |  0.09989086\n",
            "\n",
            "Step   1500: Ran 100 train steps in 78.23 secs\n",
            "Step   1500: train TripletLoss |  0.08647086\n",
            "Step   1500: eval  TripletLoss |  0.10176558\n",
            "\n",
            "Step   1600: Ran 100 train steps in 77.60 secs\n",
            "Step   1600: train TripletLoss |  0.07969842\n",
            "Step   1600: eval  TripletLoss |  0.09221563\n",
            "\n",
            "Step   1700: Ran 100 train steps in 77.20 secs\n",
            "Step   1700: train TripletLoss |  0.07516844\n",
            "Step   1700: eval  TripletLoss |  0.08442826\n",
            "\n",
            "Step   1800: Ran 100 train steps in 73.48 secs\n",
            "Step   1800: train TripletLoss |  0.05846345\n",
            "Step   1800: eval  TripletLoss |  0.07726451\n",
            "\n",
            "Step   1900: Ran 100 train steps in 77.25 secs\n",
            "Step   1900: train TripletLoss |  0.05797000\n",
            "Step   1900: eval  TripletLoss |  0.07569849\n",
            "\n",
            "Step   2000: Ran 100 train steps in 83.71 secs\n",
            "Step   2000: train TripletLoss |  0.05667935\n",
            "Step   2000: eval  TripletLoss |  0.07502411\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHAyULCAhTcb"
      },
      "source": [
        "**Model Evaluation**\n",
        "* I will utilize the Test Set which was configured earlier to determine the accuracy of the Model. Actually the Training Set only had Positive examples whereas the Test Set and y test is setup as pairs of Questions and some of which are duplicates and some are not. I will compute the Cosine Similarity of each pair, threshold it and compare the result to y test. The results are accumulated to produce the Accuracy. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRR-LHzhfmpA",
        "outputId": "e3db7a93-0819-4a2d-cb11-ee54884b6d0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#@ Loading the Saved Model:\n",
        "model = Siamese()\n",
        "model.init_from_file(\"/content/model/model.pkl.gz\")\n",
        "\n",
        "#@ Model Evaluation: \n",
        "def classify(test_Q1, test_Q2, y, threshold, model, vocab, data_generator=data_generator, batch_size=64):\n",
        "  \"\"\" Function to test the Accuracy of the Model. \"\"\"\n",
        "  accuracy = 0                                                                               # Initializing the Accuracy.\n",
        "  for i in range(0, len(test_Q1), batch_size):\n",
        "    q1, q2 = next(data_generator(test_Q1[i:i+batch_size], test_Q2[i:i+batch_size],\n",
        "                                 batch_size, vocab[\"<PAD>\"], shuffle=False))\n",
        "    y_test = y[i:i+batch_size]                                                               # Using batch size of actual output target.\n",
        "    v1, v2 = model((q1, q2))                                                                 # Using the Model.\n",
        "    for j in range(batch_size):\n",
        "      d = np.dot(v1[j], v2[j].T)                                                             # Calculating the Cosine Similarity.\n",
        "      res = d > threshold\n",
        "      accuracy += (y_test[j] == res)\n",
        "  accuracy = accuracy / len(test_Q1)\n",
        "  return accuracy\n",
        "\n",
        "#@ Computing the Accuracy of the Model:\n",
        "accuracy = classify(Q1_test, Q2_test, y_test, 0.7, model, vocab, batch_size=512)             # Calculating the Accuracy.\n",
        "print(\"Accuracy of the Model:\", accuracy) "
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the Model: 0.73671875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7_cGJx-nV7f"
      },
      "source": [
        "**Model Evaluation**\n",
        "* Now, I will test the Model using my own Questions. I will build a reverse Vocabulary that allows the map encoded Questions back to words. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQmOHJaSmSdt"
      },
      "source": [
        "#@ Model Evaluation with own Questions:\n",
        "def predict(question1, question2, threshold, model, vocab, data_generator=data_generator, verbose=False):\n",
        "  \"\"\" Function for predicting if two Questions are Duplicates. \"\"\"\n",
        "  q1 = nltk.word_tokenize(question1)                                # Tokenization.\n",
        "  q2 = nltk.word_tokenize(question2)                                # Tokenization.\n",
        "  Q1, Q2 = [], []\n",
        "  for word in q1:\n",
        "    Q1 += [vocab[word]]                                             # Encoding.\n",
        "  for word in q2:\n",
        "    Q2 += [vocab[word]]                                             # Encoding.\n",
        "  Q1, Q2 = next(data_generator([Q1], [Q2], 1, vocab[\"<PAD>\"]))\n",
        "  v1, v2 = model((Q1, Q2))                                          # Using Model.\n",
        "  d = fastnp.dot(v1[0], v2[0].T)\n",
        "  res = d > threshold\n",
        "  if (verbose):\n",
        "    print(\"Q1 = \", Q1, \"\\nQ2 = \", Q2)\n",
        "    print(\"d = \", d)\n",
        "    print(\"res = \", res)\n",
        "  return res "
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3yO1U1mrfON",
        "outputId": "27a3dd0b-d9c2-421f-e258-66eb88d8bb61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "#@ Examples of Questions:\n",
        "question1 = \"How are you?\"\n",
        "question2 = \"Are you fine?\"\n",
        "#@ Predicting the Duplicated Questions:\n",
        "example1 = predict(question1, question2, 0.7, model, vocab, verbose=True)\n",
        "print(\"Example1:\", example1, \"\\n\")\n",
        "\n",
        "#@ Example of Questions:\n",
        "question1 = \"Do you enjoy eating the dessert?\"\n",
        "question2 = \"Do you like hiking in the desert?\"\n",
        "#@ Predicting the Duplicated Questions:\n",
        "example2 = predict(question1, question2, 0.7, model, vocab, verbose=True)\n",
        "print(\"Example2:\", example2)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Q1 =  [[32 87 53 21]] \n",
            "Q2 =  [[ 520   53 6831   21]]\n",
            "d =  0.7171694\n",
            "res =  True\n",
            "Example1: True \n",
            "\n",
            "Q1 =  [[  443    53  3158  1169    78 29071    21     1]] \n",
            "Q2 =  [[  443    53    60 15323    28    78  7438    21]]\n",
            "d =  0.5682508\n",
            "res =  False\n",
            "Example2: False\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}